{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "jJ1VeajjkopW",
    "ExecuteTime": {
     "end_time": "2024-02-20T13:19:26.587547800Z",
     "start_time": "2024-02-20T13:19:26.581743200Z"
    }
   },
   "outputs": [],
   "source": [
    "# For tips on running notebooks in Google Colab, see\n",
    "# https://pytorch.org/tutorials/beginner/colab\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "i4EmBrd3kopc",
    "ExecuteTime": {
     "end_time": "2024-02-20T13:19:26.644911300Z",
     "start_time": "2024-02-20T13:19:26.586547800Z"
    }
   },
   "outputs": [
    {
     "ename": "NameNotFound",
     "evalue": "Environment `F1Env` doesn't exist.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameNotFound\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 14\u001B[0m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01moptim\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01moptim\u001B[39;00m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfunctional\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mF\u001B[39;00m\n\u001B[1;32m---> 14\u001B[0m env \u001B[38;5;241m=\u001B[39m \u001B[43mgym\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmake\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mF1Env-v0\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;66;03m# set up matplotlib\u001B[39;00m\n\u001B[0;32m     17\u001B[0m is_ipython \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124minline\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01min\u001B[39;00m matplotlib\u001B[38;5;241m.\u001B[39mget_backend()\n",
      "File \u001B[1;32mD:\\PythonProjects\\F1_RL\\venv\\Lib\\site-packages\\gymnasium\\envs\\registration.py:741\u001B[0m, in \u001B[0;36mmake\u001B[1;34m(id, max_episode_steps, autoreset, apply_api_compatibility, disable_env_checker, **kwargs)\u001B[0m\n\u001B[0;32m    738\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mid\u001B[39m, \u001B[38;5;28mstr\u001B[39m)\n\u001B[0;32m    740\u001B[0m     \u001B[38;5;66;03m# The environment name can include an unloaded module in \"module:env_name\" style\u001B[39;00m\n\u001B[1;32m--> 741\u001B[0m     env_spec \u001B[38;5;241m=\u001B[39m \u001B[43m_find_spec\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mid\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    743\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(env_spec, EnvSpec)\n\u001B[0;32m    745\u001B[0m \u001B[38;5;66;03m# Update the env spec kwargs with the `make` kwargs\u001B[39;00m\n",
      "File \u001B[1;32mD:\\PythonProjects\\F1_RL\\venv\\Lib\\site-packages\\gymnasium\\envs\\registration.py:527\u001B[0m, in \u001B[0;36m_find_spec\u001B[1;34m(env_id)\u001B[0m\n\u001B[0;32m    521\u001B[0m     logger\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[0;32m    522\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUsing the latest versioned environment `\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnew_env_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m` \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    523\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minstead of the unversioned environment `\u001B[39m\u001B[38;5;132;01m{\u001B[39;00menv_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m`.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    524\u001B[0m     )\n\u001B[0;32m    526\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m env_spec \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 527\u001B[0m     \u001B[43m_check_version_exists\u001B[49m\u001B[43m(\u001B[49m\u001B[43mns\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mversion\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    528\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m error\u001B[38;5;241m.\u001B[39mError(\n\u001B[0;32m    529\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo registered env with id: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00menv_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. Did you register it, or import the package that registers it? Use `gymnasium.pprint_registry()` to see all of the registered environments.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    530\u001B[0m     )\n\u001B[0;32m    532\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m env_spec\n",
      "File \u001B[1;32mD:\\PythonProjects\\F1_RL\\venv\\Lib\\site-packages\\gymnasium\\envs\\registration.py:393\u001B[0m, in \u001B[0;36m_check_version_exists\u001B[1;34m(ns, name, version)\u001B[0m\n\u001B[0;32m    390\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m get_env_id(ns, name, version) \u001B[38;5;129;01min\u001B[39;00m registry:\n\u001B[0;32m    391\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[1;32m--> 393\u001B[0m \u001B[43m_check_name_exists\u001B[49m\u001B[43m(\u001B[49m\u001B[43mns\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    394\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m version \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    395\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n",
      "File \u001B[1;32mD:\\PythonProjects\\F1_RL\\venv\\Lib\\site-packages\\gymnasium\\envs\\registration.py:370\u001B[0m, in \u001B[0;36m_check_name_exists\u001B[1;34m(ns, name)\u001B[0m\n\u001B[0;32m    367\u001B[0m namespace_msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m in namespace \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mns\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m ns \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    368\u001B[0m suggestion_msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m Did you mean: `\u001B[39m\u001B[38;5;132;01m{\u001B[39;00msuggestion[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m`?\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m suggestion \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m--> 370\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m error\u001B[38;5;241m.\u001B[39mNameNotFound(\n\u001B[0;32m    371\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEnvironment `\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m` doesn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt exist\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnamespace_msg\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00msuggestion_msg\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    372\u001B[0m )\n",
      "\u001B[1;31mNameNotFound\u001B[0m: Environment `F1Env` doesn't exist."
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "env = gym.make('F1Env-v0')\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if GPU is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2vKPpvNvkope",
    "ExecuteTime": {
     "start_time": "2024-02-20T13:19:26.646911Z"
    }
   },
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WPX8dPUNkopg",
    "ExecuteTime": {
     "end_time": "2024-02-20T13:19:26.649912400Z",
     "start_time": "2024-02-20T13:19:26.647911200Z"
    }
   },
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_observations, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, n_actions)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7q8Ccqrwkoph",
    "ExecuteTime": {
     "start_time": "2024-02-20T13:19:26.649912400Z"
    }
   },
   "outputs": [],
   "source": [
    "# BATCH_SIZE is the number of transitions sampled from the replay buffer\n",
    "# GAMMA is the discount factor as mentioned in the previous section\n",
    "# EPS_START is the starting value of epsilon\n",
    "# EPS_END is the final value of epsilon\n",
    "# EPS_DECAY controls the rate of exponential decay of epsilon, higher means a slower decay\n",
    "# TAU is the update rate of the target network\n",
    "# LR is the learning rate of the ``AdamW`` optimizer\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 1000\n",
    "TAU = 0.005\n",
    "LR = 1e-4\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = env.action_space.n\n",
    "# Get the number of state observations\n",
    "state, info = env.reset()\n",
    "n_observations = len(state)\n",
    "\n",
    "policy_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done  # Keeps track of the number of steps (actions selected)\n",
    "    sample = random.random()  # Generates a random sample for epsilon-greedy strategy\n",
    "\n",
    "    # Calculate the epsilon threshold for the current step using exponential decay\n",
    "    # Starts from EPS_START and decays towards EPS_END at a rate determined by EPS_DECAY\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "                    math.exp(-1. * steps_done / EPS_DECAY)\n",
    "\n",
    "    steps_done += 1  # Increment the steps_done counter\n",
    "\n",
    "    # Decision making for choosing an action based on epsilon-greedy strategy\n",
    "    if sample > eps_threshold:\n",
    "        # Exploitation: Choose the best action based on current policy\n",
    "        with torch.no_grad():  # Disable gradient calculation for inference\n",
    "            # The policy network predicts the Q-values for all actions given the current state\n",
    "            # .max(1) finds the action with the highest Q-value\n",
    "            # .indices.view(1, 1) formats the chosen action for compatibility with environment\n",
    "            return policy_net(state).max(1).indices.view(1, 1)\n",
    "    else:\n",
    "        # Exploration: Choose a random action\n",
    "        # This allows the agent to explore the action space and discover new strategies\n",
    "        return torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)\n",
    "\n",
    "\n",
    "episode_durations = []\n",
    "\n",
    "\n",
    "def plot_durations(show_result=False):\n",
    "    plt.figure(1)\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    if show_result:\n",
    "        plt.title('Result')\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4TnWsBCskopi",
    "ExecuteTime": {
     "start_time": "2024-02-20T13:19:26.650911700Z"
    }
   },
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    # Check if enough samples are available in memory to create a batch\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return  # Not enough samples, skip this round of optimization\n",
    "\n",
    "    # Sample a batch of transitions from memory\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "\n",
    "    # This clever trick transposes the batch of transitions to a Transition of batch-arrays.\n",
    "    # It effectively organizes the data for easy batch processing.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Create a mask for non-final states (i.e., states that are not the end of an episode)\n",
    "    # and a tensor for holding non-final next states\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                            batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "\n",
    "    # Concatenate all states, actions, and rewards into separate tensors\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) for each action taken in the batch\n",
    "    # This step involves forward passing the state_batch through the policy_net\n",
    "    # and using gather to select the Q-values for the actions actually taken\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Initialize a tensor for the next state values with zeros for all batch samples\n",
    "    # This will be updated with the predicted Q values for non-final states\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    # Compute V(s_{t+1}) for all non-final next states using the target network\n",
    "    # The max predicted Q value for the next states are selected with max(1).values\n",
    "    # This operation is wrapped in torch.no_grad() to prevent gradient computation\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1).values\n",
    "\n",
    "    # Compute the expected Q values for the current state-action pairs\n",
    "    # This is done by adding the (discounted) best future rewards to the immediate rewards\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute the loss between the current Q values and the expected Q values\n",
    "    # The Huber loss is used here, which is less sensitive to outliers than squared error loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Zero all gradients for the variables which the optimizer will update\n",
    "    optimizer.zero_grad()\n",
    "    # Calculate the gradients of the loss with respect to all parameters\n",
    "    # in the policy network involved in its computation\n",
    "    loss.backward()\n",
    "    # Clip gradients to prevent very large values which can destabilize training\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    # Perform a single optimization step (parameter update)\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 524
    },
    "id": "1HnWyOd8kopk",
    "outputId": "d6162154-3c07-47e7-e262-1fa28af5b410",
    "ExecuteTime": {
     "start_time": "2024-02-20T13:19:26.651910700Z"
    }
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    num_episodes = 2000\n",
    "else:\n",
    "    num_episodes = 50\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and get its state\n",
    "    state, info = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    for t in count():\n",
    "        action = select_action(state)\n",
    "\n",
    "        print(f\"Action:{action}\")\n",
    "\n",
    "        observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "\n",
    "        print(f\"Observation: {observation}\\n\"\n",
    "              f\"Reward: {reward}\\n\"\n",
    "              f\"Terminated: {terminated}\\n\"\n",
    "              f\"Truncated: {truncated}\\n\")\n",
    "\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the policy network)\n",
    "        optimize_model()\n",
    "\n",
    "        # Soft update of the target network's weights\n",
    "        # θ′ ← τ θ + (1 −τ )θ′\n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        policy_net_state_dict = policy_net.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key] * TAU + target_net_state_dict[key] * (1 - TAU)\n",
    "        target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "        if done:\n",
    "            episode_durations.append(t + 1)\n",
    "            plot_durations()\n",
    "            break\n",
    "\n",
    "print('Complete')\n",
    "plot_durations(show_result=True)\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
