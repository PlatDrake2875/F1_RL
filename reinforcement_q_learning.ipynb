{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "jJ1VeajjkopW",
    "ExecuteTime": {
     "end_time": "2024-02-13T20:16:24.768267500Z",
     "start_time": "2024-02-13T20:16:24.128415500Z"
    }
   },
   "outputs": [],
   "source": [
    "# For tips on running notebooks in Google Colab, see\n",
    "# https://pytorch.org/tutorials/beginner/colab\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "i4EmBrd3kopc",
    "ExecuteTime": {
     "end_time": "2024-02-13T20:16:26.631969900Z",
     "start_time": "2024-02-13T20:16:24.770419700Z"
    }
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if GPU is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "2vKPpvNvkope",
    "ExecuteTime": {
     "end_time": "2024-02-13T20:16:26.639828100Z",
     "start_time": "2024-02-13T20:16:26.636088900Z"
    }
   },
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "WPX8dPUNkopg",
    "ExecuteTime": {
     "end_time": "2024-02-13T20:16:26.708926300Z",
     "start_time": "2024-02-13T20:16:26.702388500Z"
    }
   },
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_observations, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, n_actions)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "7q8Ccqrwkoph",
    "ExecuteTime": {
     "end_time": "2024-02-13T20:16:29.441566500Z",
     "start_time": "2024-02-13T20:16:26.712048700Z"
    }
   },
   "outputs": [],
   "source": [
    "# BATCH_SIZE is the number of transitions sampled from the replay buffer\n",
    "# GAMMA is the discount factor as mentioned in the previous section\n",
    "# EPS_START is the starting value of epsilon\n",
    "# EPS_END is the final value of epsilon\n",
    "# EPS_DECAY controls the rate of exponential decay of epsilon, higher means a slower decay\n",
    "# TAU is the update rate of the target network\n",
    "# LR is the learning rate of the ``AdamW`` optimizer\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 1000\n",
    "TAU = 0.005\n",
    "LR = 1e-4\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = env.action_space.n\n",
    "# Get the number of state observations\n",
    "state, info = env.reset()\n",
    "n_observations = len(state)\n",
    "\n",
    "policy_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done  # Keeps track of the number of steps (actions selected)\n",
    "    sample = random.random()  # Generates a random sample for epsilon-greedy strategy\n",
    "\n",
    "    # Calculate the epsilon threshold for the current step using exponential decay\n",
    "    # Starts from EPS_START and decays towards EPS_END at a rate determined by EPS_DECAY\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "                    math.exp(-1. * steps_done / EPS_DECAY)\n",
    "\n",
    "    steps_done += 1  # Increment the steps_done counter\n",
    "\n",
    "    # Decision making for choosing an action based on epsilon-greedy strategy\n",
    "    if sample > eps_threshold:\n",
    "        # Exploitation: Choose the best action based on current policy\n",
    "        with torch.no_grad():  # Disable gradient calculation for inference\n",
    "            # The policy network predicts the Q-values for all actions given the current state\n",
    "            # .max(1) finds the action with the highest Q-value\n",
    "            # .indices.view(1, 1) formats the chosen action for compatibility with environment\n",
    "            return policy_net(state).max(1).indices.view(1, 1)\n",
    "    else:\n",
    "        # Exploration: Choose a random action\n",
    "        # This allows the agent to explore the action space and discover new strategies\n",
    "        return torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)\n",
    "\n",
    "\n",
    "episode_durations = []\n",
    "\n",
    "\n",
    "def plot_durations(show_result=False):\n",
    "    plt.figure(1)\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    if show_result:\n",
    "        plt.title('Result')\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "4TnWsBCskopi",
    "ExecuteTime": {
     "end_time": "2024-02-13T20:16:29.448931400Z",
     "start_time": "2024-02-13T20:16:29.446833800Z"
    }
   },
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    # Check if enough samples are available in memory to create a batch\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return  # Not enough samples, skip this round of optimization\n",
    "\n",
    "    # Sample a batch of transitions from memory\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "\n",
    "    # This clever trick transposes the batch of transitions to a Transition of batch-arrays.\n",
    "    # It effectively organizes the data for easy batch processing.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Create a mask for non-final states (i.e., states that are not the end of an episode)\n",
    "    # and a tensor for holding non-final next states\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                            batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "\n",
    "    # Concatenate all states, actions, and rewards into separate tensors\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) for each action taken in the batch\n",
    "    # This step involves forward passing the state_batch through the policy_net\n",
    "    # and using gather to select the Q-values for the actions actually taken\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Initialize a tensor for the next state values with zeros for all batch samples\n",
    "    # This will be updated with the predicted Q values for non-final states\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    # Compute V(s_{t+1}) for all non-final next states using the target network\n",
    "    # The max predicted Q value for the next states are selected with max(1).values\n",
    "    # This operation is wrapped in torch.no_grad() to prevent gradient computation\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1).values\n",
    "\n",
    "    # Compute the expected Q values for the current state-action pairs\n",
    "    # This is done by adding the (discounted) best future rewards to the immediate rewards\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute the loss between the current Q values and the expected Q values\n",
    "    # The Huber loss is used here, which is less sensitive to outliers than squared error loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Zero all gradients for the variables which the optimizer will update\n",
    "    optimizer.zero_grad()\n",
    "    # Calculate the gradients of the loss with respect to all parameters\n",
    "    # in the policy network involved in its computation\n",
    "    loss.backward()\n",
    "    # Clip gradients to prevent very large values which can destabilize training\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    # Perform a single optimization step (parameter update)\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 524
    },
    "id": "1HnWyOd8kopk",
    "outputId": "d6162154-3c07-47e7-e262-1fa28af5b410"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action:tensor([[0]], device='cuda:0')\n",
      "Observation: [-0.03249109 -0.19444314  0.01266395  0.3309469 ]\n",
      "Reward: 1.0\n",
      "Terminated: False\n",
      "Truncated: False\n",
      "\n",
      "Action:tensor([[1]], device='cuda:0')\n",
      "Observation: [-0.03637995  0.00049628  0.01928289  0.04228433]\n",
      "Reward: 1.0\n",
      "Terminated: False\n",
      "Truncated: False\n",
      "\n",
      "Action:tensor([[0]], device='cuda:0')\n",
      "Observation: [-0.03637003 -0.19489679  0.02012858  0.34098828]\n",
      "Reward: 1.0\n",
      "Terminated: False\n",
      "Truncated: False\n",
      "\n",
      "Action:tensor([[1]], device='cuda:0')\n",
      "Observation: [-4.0267963e-02 -6.6930283e-05  2.6948344e-02  5.4720085e-02]\n",
      "Reward: 1.0\n",
      "Terminated: False\n",
      "Truncated: False\n",
      "\n",
      "Action:tensor([[0]], device='cuda:0')\n",
      "Observation: [-0.0402693  -0.1955647   0.02804274  0.35578212]\n",
      "Reward: 1.0\n",
      "Terminated: False\n",
      "Truncated: False\n",
      "\n",
      "Action:tensor([[1]], device='cuda:0')\n",
      "Observation: [-0.04418059 -0.00085246  0.03515839  0.07207207]\n",
      "Reward: 1.0\n",
      "Terminated: False\n",
      "Truncated: False\n",
      "\n",
      "Action:tensor([[0]], device='cuda:0')\n",
      "Observation: [-0.04419765 -0.19646035  0.03659983  0.37563702]\n",
      "Reward: 1.0\n",
      "Terminated: False\n",
      "Truncated: False\n",
      "\n",
      "Action:tensor([[1]], device='cuda:0')\n",
      "Observation: [-0.04812685 -0.00187686  0.04411257  0.0947152 ]\n",
      "Reward: 1.0\n",
      "Terminated: False\n",
      "Truncated: False\n",
      "\n",
      "Action:tensor([[1]], device='cuda:0')\n",
      "Observation: [-0.04816439  0.19258599  0.04600687 -0.18373042]\n",
      "Reward: 1.0\n",
      "Terminated: False\n",
      "Truncated: False\n",
      "\n",
      "Action:tensor([[0]], device='cuda:0')\n",
      "Observation: [-0.04431267 -0.00316302  0.04233227  0.12310366]\n",
      "Reward: 1.0\n",
      "Terminated: False\n",
      "Truncated: False\n",
      "\n",
      "Action:tensor([[0]], device='cuda:0')\n",
      "Observation: [-0.04437593 -0.19886509  0.04479434  0.42883575]\n",
      "Reward: 1.0\n",
      "Terminated: False\n",
      "Truncated: False\n",
      "\n",
      "Action:tensor([[1]], device='cuda:0')\n",
      "Observation: [-0.04835323 -0.00440519  0.05337105  0.15060362]\n",
      "Reward: 1.0\n",
      "Terminated: False\n",
      "Truncated: False\n",
      "\n",
      "Action:tensor([[0]], device='cuda:0')\n",
      "Observation: [-0.04844134 -0.20024915  0.05638313  0.4596349 ]\n",
      "Reward: 1.0\n",
      "Terminated: False\n",
      "Truncated: False\n",
      "\n",
      "Action:tensor([[1]], device='cuda:0')\n",
      "Observation: [-0.05244632 -0.00596763  0.06557582  0.18524358]\n",
      "Reward: 1.0\n",
      "Terminated: False\n",
      "Truncated: False\n",
      "Action:tensor([[0]], device='cuda:0')\n",
      "Observation: [-0.05256567 -0.20196356  0.0692807   0.49787107]\n",
      "Reward: 1.0\n",
      "Terminated: False\n",
      "Truncated: False\n",
      "\n",
      "Action:tensor([[1]], device='cuda:0')\n",
      "Observation: [-0.05660494 -0.00788334  0.07923812  0.22780135]\n",
      "Reward: 1.0\n",
      "Terminated: False\n",
      "Truncated: False\n",
      "\n",
      "Action:tensor([[1]], device='cuda:0')\n",
      "Observation: [-0.05676261  0.18602209  0.08379415 -0.03887252]\n",
      "Reward: 1.0\n",
      "Terminated: False\n",
      "Truncated: False\n",
      "\n",
      "Action:tensor([[0]], device='cuda:0')\n",
      "Observation: [-0.05304217 -0.01019528  0.08301669  0.2790275 ]\n",
      "Reward: 1.0\n",
      "Terminated: False\n",
      "Truncated: False\n",
      "\n",
      "Action:tensor([[1]], device='cuda:0')\n",
      "Observation: [-0.05324607  0.18365033  0.08859725  0.01363935]\n",
      "Reward: 1.0\n",
      "Terminated: False\n",
      "Truncated: False\n",
      "\n",
      "Action:tensor([[0]], device='cuda:0')\n",
      "Observation: [-0.04957307 -0.01262315  0.08887003  0.33290836]\n",
      "Reward: 1.0\n",
      "Terminated: False\n",
      "Truncated: False\n",
      "\n",
      "Action:tensor([[1]], device='cuda:0')\n",
      "Observation: [-0.04982553  0.18112889  0.0955282   0.06952062]\n",
      "Reward: 1.0\n",
      "Terminated: False\n",
      "Truncated: False\n",
      "\n",
      "Action:tensor([[1]], device='cuda:0')\n",
      "Observation: [-0.04620295  0.3747607   0.09691861 -0.19156024]\n",
      "Reward: 1.0\n",
      "Terminated: False\n",
      "Truncated: False\n",
      "\n",
      "Action:tensor([[0]], device='cuda:0')\n",
      "Observation: [-0.03870774  0.1783955   0.0930874   0.13005477]\n",
      "Reward: 1.0\n",
      "Terminated: False\n",
      "Truncated: False\n",
      "\n",
      "Action:tensor([[1]], device='cuda:0')\n",
      "Observation: [-0.03513983  0.3720692   0.0956885  -0.13186987]\n",
      "Reward: 1.0\n",
      "Terminated: False\n",
      "Truncated: False\n",
      "\n",
      "Action:tensor([[0]], device='cuda:0')\n",
      "Observation: [-0.02769844  0.17571609  0.09305111  0.18940195]\n",
      "Reward: 1.0\n",
      "Terminated: False\n",
      "Truncated: False\n",
      "\n",
      "Action:tensor([[1]], device='cuda:0')\n",
      "Observation: [-0.02418412  0.36939216  0.09683914 -0.07253776]\n",
      "Reward: 1.0\n",
      "Terminated: False\n",
      "Truncated: False\n",
      "\n",
      "Action:tensor([[0]], device='cuda:0')\n",
      "Observation: [-0.01679628  0.17302486  0.09538839  0.24905935]\n",
      "Reward: 1.0\n",
      "Terminated: False\n",
      "Truncated: False\n",
      "\n",
      "Action:tensor([[1]], device='cuda:0')\n",
      "Observation: [-0.01333578  0.36666432  0.10036957 -0.01207773]\n",
      "Reward: 1.0\n",
      "Terminated: False\n",
      "Truncated: False\n",
      "\n",
      "Action:tensor([[0]], device='cuda:0')\n",
      "Observation: [-0.0060025   0.17025688  0.10012802  0.31050986]\n",
      "Reward: 1.0\n",
      "Terminated: False\n",
      "Truncated: False\n",
      "\n",
      "Action:tensor([[1]], device='cuda:0')\n",
      "Observation: [-0.00259736  0.3638203   0.10633822  0.05100742]\n",
      "Reward: 1.0\n",
      "Terminated: False\n",
      "Truncated: False\n",
      "\n",
      "Action:tensor([[0]], device='cuda:0')\n",
      "Observation: [0.00467905 0.167347   0.10735837 0.37525722]\n",
      "Reward: 1.0\n",
      "Terminated: False\n",
      "Truncated: False\n",
      "\n",
      "Action:tensor([[1]], device='cuda:0')\n",
      "Observation: [0.00802599 0.3607933  0.11486351 0.11826118]\n",
      "Reward: 1.0\n",
      "Terminated: False\n",
      "Truncated: False\n",
      "\n",
      "Action:tensor([[1]], device='cuda:0')\n",
      "Observation: [ 0.01524185  0.5540981   0.11722873 -0.13608976]\n",
      "Reward: 1.0\n",
      "Terminated: False\n",
      "Truncated: False\n",
      "\n",
      "Action:tensor([[0]], device='cuda:0')\n",
      "Observation: [0.02632382 0.3575092  0.11450694 0.1911561 ]\n",
      "Reward: 1.0\n",
      "Terminated: False\n",
      "Truncated: False\n",
      "\n",
      "Action:tensor([[1]], device='cuda:0')\n",
      "Observation: [ 0.033474    0.5508227   0.11833006 -0.06332367]\n",
      "Reward: 1.0\n",
      "Terminated: False\n",
      "Truncated: False\n",
      "\n",
      "Action:tensor([[0]], device='cuda:0')\n",
      "Observation: [0.04449046 0.35422048 0.11706359 0.26422536]\n",
      "Reward: 1.0\n",
      "Terminated: False\n",
      "Truncated: False\n",
      "\n",
      "Action:tensor([[1]], device='cuda:0')\n",
      "Observation: [0.05157486 0.5474939  0.12234809 0.01063756]\n",
      "Reward: 1.0\n",
      "Terminated: False\n",
      "Truncated: False\n",
      "\n",
      "Action:tensor([[0]], device='cuda:0')\n",
      "Observation: [0.06252474 0.35084906 0.12256084 0.33928052]\n",
      "Reward: 1.0\n",
      "Terminated: False\n",
      "Truncated: False\n",
      "\n",
      "Action:tensor([[1]], device='cuda:0')\n",
      "Observation: [0.06954172 0.54403335 0.12934646 0.08762054]\n",
      "Reward: 1.0\n",
      "Terminated: False\n",
      "Truncated: False\n",
      "\n",
      "Action:tensor([[0]], device='cuda:0')\n",
      "Observation: [0.08042239 0.34731743 0.13109887 0.41815138]\n",
      "Reward: 1.0\n",
      "Terminated: False\n",
      "Truncated: False\n",
      "Action:tensor([[1]], device='cuda:0')\n",
      "Observation: [0.08736874 0.54036164 0.13946189 0.16950266]\n",
      "Reward: 1.0\n",
      "Terminated: False\n",
      "Truncated: False\n",
      "\n",
      "Action:tensor([[1]], device='cuda:0')\n",
      "Observation: [ 0.09817597  0.7332405   0.14285195 -0.07613762]\n",
      "Reward: 1.0\n",
      "Terminated: False\n",
      "Truncated: False\n",
      "\n",
      "Action:tensor([[0]], device='cuda:0')\n",
      "Observation: [0.11284078 0.5363903  0.1413292  0.2579857 ]\n",
      "Reward: 1.0\n",
      "Terminated: False\n",
      "Truncated: False\n",
      "\n",
      "Action:tensor([[1]], device='cuda:0')\n",
      "Observation: [0.12356859 0.7292415  0.1464889  0.01300581]\n",
      "Reward: 1.0\n",
      "Terminated: False\n",
      "Truncated: False\n",
      "\n",
      "Action:tensor([[0]], device='cuda:0')\n",
      "Observation: [0.13815342 0.53235555 0.14674902 0.3480855 ]\n",
      "Reward: 1.0\n",
      "Terminated: False\n",
      "Truncated: False\n",
      "\n",
      "Action:tensor([[1]], device='cuda:0')\n",
      "Observation: [0.14880052 0.7251187  0.15371074 0.10503813]\n",
      "Reward: 1.0\n",
      "Terminated: False\n",
      "Truncated: False\n",
      "\n",
      "Action:tensor([[0]], device='cuda:0')\n",
      "Observation: [0.1633029  0.52816635 0.1558115  0.4419967 ]\n",
      "Reward: 1.0\n",
      "Terminated: False\n",
      "Truncated: False\n",
      "\n",
      "Action:tensor([[1]], device='cuda:0')\n",
      "Observation: [0.17386623 0.7207799  0.16465144 0.20219982]\n",
      "Reward: 1.0\n",
      "Terminated: False\n",
      "Truncated: False\n",
      "\n",
      "Action:tensor([[1]], device='cuda:0')\n",
      "Observation: [ 0.18828182  0.9132112   0.16869543 -0.03435425]\n",
      "Reward: 1.0\n",
      "Terminated: False\n",
      "Truncated: False\n",
      "\n",
      "Action:tensor([[0]], device='cuda:0')\n",
      "Observation: [0.20654605 0.71612227 0.16800834 0.30644408]\n",
      "Reward: 1.0\n",
      "Terminated: False\n",
      "Truncated: False\n",
      "\n",
      "Action:tensor([[1]], device='cuda:0')\n",
      "Observation: [0.2208685  0.9085015  0.17413722 0.07110071]\n",
      "Reward: 1.0\n",
      "Terminated: False\n",
      "Truncated: False\n",
      "\n",
      "Action:tensor([[0]], device='cuda:0')\n",
      "Observation: [0.23903853 0.71136624 0.17555924 0.41326955]\n",
      "Reward: 1.0\n",
      "Terminated: False\n",
      "Truncated: False\n",
      "\n",
      "Action:tensor([[1]], device='cuda:0')\n",
      "Observation: [0.25326586 0.9036218  0.18382463 0.18066865]\n",
      "Reward: 1.0\n",
      "Terminated: False\n",
      "Truncated: False\n",
      "\n",
      "Action:tensor([[1]], device='cuda:0')\n",
      "Observation: [ 0.27133828  1.0957026   0.18743801 -0.04885771]\n",
      "Reward: 1.0\n",
      "Terminated: False\n",
      "Truncated: False\n",
      "\n",
      "Action:tensor([[0]], device='cuda:0')\n",
      "Observation: [0.29325235 0.89845663 0.18646085 0.29661375]\n",
      "Reward: 1.0\n",
      "Terminated: False\n",
      "Truncated: False\n",
      "\n",
      "Action:tensor([[1]], device='cuda:0')\n",
      "Observation: [0.31122148 1.090499   0.19239312 0.06804571]\n",
      "Reward: 1.0\n",
      "Terminated: False\n",
      "Truncated: False\n",
      "Action:tensor([[0]], device='cuda:0')\n",
      "Observation: [0.33303145 0.8932141  0.19375403 0.41472834]\n",
      "Reward: 1.0\n",
      "Terminated: False\n",
      "Truncated: False\n",
      "\n",
      "Action:tensor([[0]], device='cuda:0')\n",
      "Observation: [0.35089573 0.69594985 0.2020486  0.7616959 ]\n",
      "Reward: 1.0\n",
      "Terminated: False\n",
      "Truncated: False\n",
      "\n",
      "Action:tensor([[1]], device='cuda:0')\n",
      "Observation: [0.36481473 0.88780063 0.21728252 0.5387728 ]\n",
      "Reward: 1.0\n",
      "Terminated: True\n",
      "Truncated: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    num_episodes = 600\n",
    "else:\n",
    "    num_episodes = 50\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and get its state\n",
    "    state, info = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    for t in count():\n",
    "        action = select_action(state)\n",
    "\n",
    "        print(f\"Action:{action}\")\n",
    "\n",
    "        observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "\n",
    "        print(f\"Observation: {observation}\\n\"\n",
    "              f\"Reward: {reward}\\n\"\n",
    "              f\"Terminated: {terminated}\\n\"\n",
    "              f\"Truncated: {truncated}\\n\")\n",
    "\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the policy network)\n",
    "        optimize_model()\n",
    "\n",
    "        # Soft update of the target network's weights\n",
    "        # θ′ ← τ θ + (1 −τ )θ′\n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        policy_net_state_dict = policy_net.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key] * TAU + target_net_state_dict[key] * (1 - TAU)\n",
    "        target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "        if done:\n",
    "            episode_durations.append(t + 1)\n",
    "            plot_durations()\n",
    "            break\n",
    "\n",
    "print('Complete')\n",
    "plot_durations(show_result=True)\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
